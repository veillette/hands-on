# Scientific Thinking and Experimenting\label{ch4}
## Observations and Models\label{sec4_1}
In this chapter we briefly review the nature of scientific activity in the hope that the procedures used in various types of experimenting will be seen to arise naturally from the problems that are encountered.
To understand the nature of scientific thinking, it helps to go back to fundamentals and pretend that we are inventing a new area of scientific study right from the beginning.
\subsection*{Identification of Significant Variables}
As we encounter through observation a totally new phenomenon, our natural first question is -- What causes this?
The question was asked with respect to the diffraction of light, radioactivity, superconductivity, pulsars, and every other physical phenomenon.
It is still being asked with respect to the nature of elementary particles, climatic change, cancer, and many other topics.
Asking questions about causes can lead to philosophic difficulties, and it is better to recognize that our natural questions about causes and explanations for phenomena are really questions about the relationships between observed variables.
The flow of electrical current through a conductor, for example, can be observed by using ordinary lab equipment to depend strongly on the potential difference across it and not at all on whether the conductor is oriented North-South or East-West, and this observation can be used to guide future study.
This may seem like a foolishly oversimplified example, but at the frontiers of scientific work when we know nothing about a new phenomenon,
we may have to consider a wide range of possibilities.
Normally, the first phase of research on a totally new phenomenon consists of a search for the variables that seem to be related.
By identifying these significant variables, we narrow the field of investigation to practical levels and facilitate
continued work at both experimental and theoretical levels.

It is interesting that, at this primary stage of scientific development, we can make relatively definite statements because we are talking, about actual observations.
This accounts for the reputation of scientific activities that they lead to ``scientific truth'' about the universe.
The claim must be restricted to the early, diagnostic stage at which we identify the significant variables.
Following this, come later stages in which we deal with a totally different type of activity that involves a much lower level of certainty.

\subsection*{Concept of a Model}
After we have analyzed a new phenomenon and are aware of the significant variables, we can proceed to the next level of sophistication, To illustrate this stage, consider an elementary example.
Suppose we were going to paint a wall and wished to know the amount of paint to order.
We would have to
know the area of the wall, so what would we do?
The natural reaction would be to measure the length of the wall and its height and then multiply the two numbers together.
But what would that give us?
And why would we think that the numerical product has anything to do with the wall?
When we multiply
these two numbers together, we do obtain something, but it is the area of the completely imaginary rectangle that is defined by the two lengths.
This imaginary rectangle may or may not have any relationship to the wall.
The important thing to notice is that we are dealing with two completely different categories.
First, there is the real wall whose area we need.
Second, there is a
completely invented, conceptual rectangle that is constructed from definitions exists in our imagination only and, in the present case, is specified by the two measured lengths.
We are commonly insensitive to this important distinction because we are all so familiar with the concept of rectangles that a simple, almost subconscious, glance at the wall reassures us that a rectangle
is a satisfactory representation of the wall.

But suppose we were not able to make that judgment.
Suppose we were blind and had done nothing more than measure the base and one side of the wall without thinking about angles or any other property of the wall.
We could multiply our two dimensions to obtain an area that had no relevance at all to the
wall if the wall happened to have the shape of a parallelogram.
To avoid that kind of error, as blind experimenters, we would have to recognize the necessity to check that the imaginary rectangle defined by the two dimensions is compared sufficiently closely with the actual wall.
To do this, we would have to know the various properties of rectangles, and we would have to compare with the actual wall as many of these properties as possible.
For example, we could test such properties as straightness of sides, right-angle comers, equality of diagonals, and so on.
Only after a sufficient number of properties had been compared between the rectangular construct and the real wall, and found to correspond adequately, could we have faith that the area of the imaginary rectangle was a good enough
approximation to the actual area of the real wall.

The distinction we have been discussing is most significant and must be borne clearly in mind as we pursue scientific work.
In all areas of scientific study we shall find, on the one hand, the real world and our perceptions of it, and, on the other hand, hypothetical, imaginary constructs fabricated out of sets of definitions.
Such a construct is often called a \textbf{model} of the situation, and the use of models is almost universal in our thinking, whether scientific or nonscientific.
The painter contemplating the task of painting the wall has in mind the imaginary rectangle.
In addition to the real flower that is being studied, a botanist is aware of the concept of the particular species to which the flower is assigned; in contrast to the real flower, the species is a construct that has been defined by a standardized list of properties.
Economists studying the economy of a country construct models that consist of a set of definitions and equations and have properties, they hope, that are similar to the actual properties of the real economy.
As shorthand descriptions of systems, models give us a framework for thought and communication, a basis
for calculation, a guide for future study, and many other advantages.

The use of models is universal in scientific work.
Models come in many different kinds and they serve many different purposes, but we must remember their most important characteristic -- they \textit{all} are invented concepts.
They are constructed so that their properties correspond as closely as possible to those of the real world, but no model can ever be an exact replica of its real counterpart.
Models belong to different categories; a wall cannot actually \textit{be} a rectangle, nor a wheel a circle.
The properties of a model, however, may be \textit{similar} to the properties of the real world, and in general terms the usefulness of a model depends on the extent that its properties do correspond with those of the real world.

\subsection*{Comparison Between Models and the Real World}
At the beginning of an experimental study, we are usually unaware of the extent to which the properties of our model and its real-world counterpart correspond.
It is necessary, as a basis for all later work, to start by testing the model against the real system.
Only if the properties of the model are shown experimentally to be adequately in correspondence with those of the real system are we justified, like the painter about to order paint, in proceeding to the next step.

Notice that, to be useful scientifically, a model or concept must be actually testable against observation.
Thus, a proposition regarding the number of angels who can dance on the head of a pin cannot qualify as science.
This is not to say that the only useful ideas are those that can be tested against experience, only that other propositions do not come under the heading of science.
Those other propositions may perhaps be perfectly valid as mathematical or philosophical statements, or as aesthetic or ethical judgments.

\subsection*{Refinement of Models}
In general, an experimental situation contains, first, the system itself, and, second, a model or models of the system.
Whatever else is involved, it is an essential part of the experimenter's task to test the properties of the model against the properties of the real system.
In principle, our model will inevitably be incomplete and inaccurate.
For example, let us return to the problem of ordering paint for the wall.
If, as blind painters, we test the properties of rectangles with increasing precision against those of the actual wall, we inevitably reach a point at which we begin to find discrepancies.
If, at that point, a simple rectangle is not a good enough model of the wall at the new level of precision, we must modify the model in an effort to improve the match between the model and the wall.
To do this, we could progressively make small changes in angles or lengths and hope that the areas calculated for the revised models will provide increasingly accurate estimates for the actual area of the real wall.
Even with these adjustments, the model remains an invented concept, and the area calculated from the model belongs to the model and not to the wall.

In scientific work, generally, we should feel free to change our models any time the need arises.
The model is our construct, to begin with, and it is only an idea that exists in our heads.
In contemplating change, our only consideration is the basic usefulness of the idea and its improved utility if it is altered in any way.
Because it is presumably impossible to construct for a piece of the natural world a verbal or mathematical description that is the exact and total equivalent of the real thing, a process of continued refinement and eventual replacement of models must be accepted as the natural course of events.
It is the normal business of scientists, whether ``pure,'' technological, or social, to use the process of comparing models and systems in a continuous
search for improvement in models.
This is usually not an easy process.
The models we have now are as good as generations of intelligent and hard-working scientists in the past have been able to make them.
We should consider ourselves fortunate in our professional work if we are able to make a
few small improvements to existing models.
Major revisions or the introduction of completely new models are rare and tend to be associated with Nobel Prizes.

On the other hand, we need not be totally preoccupied with improving models.
Even If no model can be the exact equivalent of the real thing, the properties of our models and systems can frequently correspond sufficiently well for our purposes.
If so, we need not be excessively concerned with the
remaining defects.
We can proceed confidently with our particular task, provided that we remember periodically to recheck the situation and confirm the continued suitability of the model.
It is not appropriate to think about the ``rightness'' or ``wrongness'' of models.
We cannot claim that a model is ``correct,'' only that it is ``adequate,'' or ``suitable,'' or ``appropriate'' for the purposes in hand.

\subsection*{Model Building in the History of Science}
It is possible to gain the impression from the foregoing discussion that in scientific development there is some kind of unique sequence that starts with observation and ends with a satisfactory model.
Indeed, scientific thinking has quite frequently progressed in this way, but the sequence is not invariable.
There are many examples of a basic, invented idea, the foundation of a model, that was the fruit of pure speculation by the originator, without awareness of the observations that could be directly associated with the conjecture.
We can recall, as examples, de Broglie's speculation on the wave model of matter, which was published in 1924 before any of the relevant phenomena were observed directly, and Fermi's invention of the concept of the neutrino almost 40 years before the particle itself was directly observed.
There is, no single process of scientific development, no single ``scientific method.'' Ideas and observations tend to shuffle forward roughly together but with no automatic leadership from one or the other.
Regardless of the precise order of development, one point remains invariable -- the fundamental activity in scientific experimenting is to compare the properties of models with the corresponding properties of the real world.

We have not discussed at all the processes by which new ideas are introduced to serve as a basis for totally new theories.
Sometimes an existing idea can undergo a process of continued refinement and attain closer and closer correspondence with observation without any alteration of the basic concepts on which the theory is founded (for example, the Ptolemaic theory of planetary epicycles).
On the other hand, a theory such as Einstein's theory of general relativity or Schr{\"o}dinger's wave mechanics can be introduced only after a completely radical revision of basic concepts and ways of thinking -- not a simple process.
The manner in which such major revolutions in scientific thinking have occurred is described in the books by Kuhn, Cohen, and Harr{\'e} that are listed in the Bibliography.

One might think that, following such major revolutions, a superseded model or theory would be immediately discarded to make way for its successor.
Indeed, many models or theories have found no lasting usefulness -- one does not hear too much these days about phlogiston, or about earth, fire, air, and water -- but this is not always the case.
Superseded models have quite often sufficiently close correspondence with the system that, usually on account of simplicity, they continue to be very useful.
If one wishes to determine the depth of a well by dropping a stone into the water, one does not need to use Einstein's general theory of relativity as a model for gravitational acceleration.
The more sophisticated model must be used when circumstances demand it -- for example when we wish to predict the motion of the planet Mercury.

\subsection*{Detailed Comparison Between Models and Systems}
To summarize our development so far, we have four ingredients in the scientific recipe: (1) observation, (2) an idea constructed in our imagination, (3) the process of comparing the properties of the idea with those of the real world, and (4) the possibility of modifying the idea progressively to improve the fit between the model and the system.
We now turn our attention to the
actual procedures by which we can compare the properties of models and systems.
It is not sufficient to have a vague pictorial concept of the situation; to supply an adequate basis for comparison we must be as explicit as possible.
This normally requires quantitative observation of the system and
mathematical procedures for specifying the model.
Let us consider some specific examples and investigate the various levels of sophistication in the methods for constructing models and comparing them with real systems.

Consider an elastic band, suspended from its upper end, from the lower end of which we can hang weights.
The most primitive form of construct with respect to the properties of the system would be a verbal description of its behavior.
We could say something like: As I hang more weights at the bottom of the elastic band, it stretches farther.
This verbal description could prompt us to invent the general concept ``springiness'' to serve as a model.
But if we wish to refine the model to make it more useful for detailed comparison with observation, our purely verbal methods of description start to fail us; we cannot refine such a vague concept as springiness without resorting to the precision of description that is available in numerical and mathematical modes of expression.
We would then make a series of measurements of the extension of the elastic band as a function of load, hoping that they will suggest a more explicit concept.
We would obtain a set of measurements such as those shown in Table {ref}`table4_1`

```{list-table}
:table4_1
:header-rows: 1

* - \hline
    Load (kg)
  - Extension (m)


    \hline
    `0.05`
  - `0.03\pm 0.01`


    `0.10`
  - `0.04`


    `0.15`
  - `0.08`


    `0.20`
  - `0.13`


    `0.25`
  - `0.19`


    `0.30`
  - `0.30`


    `0.35`
  - `0.34`


    `0.40`
  - `0.38`


    `0.45`
  - `0.39`


    \hline
```
(Notice that for simplification we are pretending to know the weights exactly so that we can ignore the uncertainty in them.
The values of extension for the rubber band are measurements made by us, and so the uncertainty must be included.)

Now that we have the measurements, do they give us a complete and adequate description of the results?
Not really.
It is difficult to judge the behavior of a system from a set of numbers in a table; some form of visual presentation is much superior.
A simple graph of the observations can comprise all the information contained in the table and can, in addition, confer the enormous benefit of facilitating visual judgment of the results.
Such a diagram is shown in Figure {ref}`Fig4_1` in which we have plotted, in addition to the central values of the measured variables, the actual intervals over which the measurements of extension are uncertain.
```{figure} figures/ch4/Fig4_1.png
:Fig4_1
```

In Figure {ref}`Fig4_1` we have done nothing more than plot the observations on the graph.
At this stage the set of observations is the only thing we have, and there is no justification for putting anything else on the graph.

This completes the first stage of the process, that is, observation.
We must now undertake the next stage, in which we construct a model, or models, of the system.

## Construction of Models\label{sec4_2}
The type of process required at the various stages depends much on the particular experiment.
For example, we may be experimenting on a phenomenon that is being observed for the first time and for which there are no existing ideas.
In such a case, our task would be to identify the significant variables and possibly generate some kind of model.
Or we may be working on some relatively familiar phenomenon, in which case we would probably have some existing proposal or theory that could be applied to our system, thus creating a model.
Whatever the circumstances, we draw a distinction between models that are \textbf{empirical} and models that are \textbf{theoretical}.
The word \textit{empirical} means that models of this type are based solely on the observations themselves, without any reference to the detailed, internal operation of the system.
The processes by which we can generate empirical models and the usefulness of such models is described as we proceed.
A theoretical model is constructed more generally, not just for one particular set of observations,
and is based on some basic concept or principle about the actual mode of operation of the system.
The nature of theoretical models and their usefulness is also described.
We consider each type in turn.

\subsection*{Empirical Models}
Assume that we have made a set of observations on a system for which there is no existing model.
All we have is a set of observations on some properties of the system.
It could be the load versus extension measurements on our elastic band, and the results probably take the form of a graph like that in Figure {ref}`Fig4_1`.
Our problem is to construct a suitable model.
What can we do?
There are several possibilities, and we consider them in order of increasing sophistication.

\noindent\textbf{Verbal statement.} The simplest model of all is a simple verbal description of the variation.
We could say something like: The extension increases smoothly with load in an S-shaped curve.
Notice that even this simple sentence is a construct.
As soon as we stop talking about the individual observations and start talking about the whole variation of extension with load, we
have made the transition from statements about particular observations to constructed presumptions about the behavior of the system.
Even such a vague proposition as the foregoing statement could, on closer measurement, turn out to be unsatisfactory.
Perhaps, for example, the variation is really
stepwise rather than smooth.
The constructed nature of even such simple statements is stressed here as a reminder that we must always be aware of the distinction between statements about the observations themselves and statements that sound as if they were about the observations but are actually statements about our \textit{ideas} concerning the observations.

\noindent\textbf{Drawing a smooth curve through the points.} The next stage of sophistication in model construction is represented by a process that is so commonly carried out (usually without due regard to its significance) that its name is used as the heading for this section.
As we view the graph of observations initially (Figure {ref}`Fig4_1`), we must remember that it contains the observational points \textit{and nothing else}; we have no basis yet for putting anything else on the diagram.
There will inevitably be some scatter in the points because of their inherent uncertainty, but it is possible to base the model construction on the single basic assumption that, uncertainty and scatter notwithstanding, the actual behavior of the system is smooth and continuous.
This is \textit{our} concept, or idea, and as we draw a smooth curve (Figure {ref}`Fig4_2`) through the points, we assume that it is valid to apply that concept to our system.

The assumption of smooth, continuous behavior can be valid to a high degree of accuracy for many systems.
An example is planetary motion, for which many of the procedures for treating observations were first devised.
```{figure} figures/ch4/Fig4_2.png
:A smooth curve through the observations.
:Fig4_2
```
The responsibility for deciding to assume smooth and regular behavior lies with the experimenter, who should make the assumption only if familiarity with the system leads to the carefully considered conviction that it is valid.

The benefits of assuming regular behavior and drawing a smooth curve through the points can be substantial.
One of the most obvious benefits is associated with interpolation and extrapolation.
Consider that we have the set of observations shown originally in Figure {ref}`Fig4_1` and that we have drawn a smooth curve through the points as shown in Figure {ref}`Fig4_2`.
Our knowledge of
the system is good at the points at which measurements have actually been made, but if we want to find the value of the extension at a load intermediate between two of the measured values, we have a problem.
We could go back to the apparatus and make the desired measurement but for many reasons
this course of action could be either impossible or undesirable.
We are then left with the possibility of only inferring the desired value on the basis of the existing measurements.
The smooth curve provides one way of doing so, as shown in Figure {ref}`Fig4_3`.
We must remember that the answer obtained by interpolation is an inferred value that depends on the decision to draw a particular
smooth curve.
```{figure} figures/ch4/Fig4_3.png
:The use of a smooth curve to obtain values.
:Fig4_3
```

Likewise, it is possible to use a smooth curve to extrapolate \textit{beyond} the existing range of values, as shown in Figure {ref}`Fig4_4`.
Such a procedure enables us to make a guess at values outside the measured range, but the validity of the procedure is obviously much more limited than was the case for interpolation.
Before proceeding with extrapolation, we must have very good reasons
to believe that the system's behavior remains regular beyond the measured range.
Smooth variation inside the measured range does not by itself offer any guarantees about a wider range of behavior (Figure {ref}`Fig4_5`).

Mathematical methods for interpolation and extrapolation are given in Appendix {ref}`A3`, and they can be used to obtain interpolated and extrapolated values by calculation without actually drawing the smooth curve.
Such methods still depend on the assumption of smooth, regular behavior of the system, and the inferred values draw their validity from the reliability of that assumption.

Because the validity of interpolation and extrapolation is limited by the assumption of smooth and regular behavior, opportunities for error abound.
If, for example, we were to offer someone the graph of temperature versus time (Figure {ref}`Fig4_6`) without specifying the system and ask that person to infer the value of temperature for a time halfway between two measured values,
```{figure} figures/ch4/Fig4_4.png
:The use of a smooth curve to obtain values by extrapolation.
:Fig4_4
```
the usual answer would be to draw the smooth curve and obtain the interpolated value as shown on the graph.
We could then reveal that the graph depicts the noonday temperatures for the first few days of this month and that we were asking for a temperature at midnight.
Likewise, people who have belief in the infallible validity of extrapolation can be asked why they have not made a fortune on the stock market.

Before we leave the topic of drawing smooth curves through points, one final procedure deserves mention.
We commonly encounter graphs in which the points have been connected by straight-line segments, as shown in Figure {ref}`Fig4_7`.
Spreadsheets or computer graphics programs usually automatically draw line graphs in this form.
How are we to interpret such a diagram?
Surely, we are not being asked to believe that these segments represent the system's actual behavior between the measured points.
The only possible benefit seems to be to supply some kind of emphasis.
In a diagram containing a number of possibly intersecting graphs, the segments sometimes help identify the various graphs.
However, such segments represent satisfactorily neither of the two basic ingredients of experiments, observations, and models, so their use is rarely beneficial, and they can be misleading.
For scientific work, they are not recommended except in special cases.
The way in which the common spreadsheet programs can be adapted for our purposes is described later.
```{figure} figures/ch4/Fig4_5.png
:Extrapolation is not one of the exact sciences.
:Fig4_5
```

\noindent\textbf{Function Finding.} As a more sophisticated form of drawing smooth curves through points, we can use various mathematical methods to find various analytical functions the graphs of which match, to a greater or lesser extent, the variation of the measured values.
Obviously, despite all the mathematical sophistication that may be involved, such procedures still depend for their validity on the basic assumption of regular behavior in the system; the curves and functions are our concept of the behavior of the system.
Nevertheless, functions generated empirically to fit sets of observations can be useful.
As mathematical models of the system, they can, be used with varying precision, to obtain inferred values for some characteristic of the system by interpolation
and extrapolation.

It is important to remember clearly that interpolation and extrapolation using an empirical function depend on the validity of that particular function as a model for the system.
This does not usually pose too much of a problem for interpolation, where good knowledge of the actual behavior of the system is available on both sides of the interpolated value.
```{figure} figures/ch4/Fig4_6.png
:Potentially fallacious use of interpolation.
:Fig4_6
```
Extrapolation is a different matter.
We usually notice this when we are dealing with extrapolation in
time; forecasting is an uncertain business.
We can forecast accurately the time of sunset a week away because the relevant models are very good.
We have much less success in forecasting the weather in the coming weeks because our models are much less satisfactory, and forecasting in other areas, such as the
stock market, proves to be almost impossible.

It suffices at present to note the possibility of constructing empirical mathematical models of systems.
The methods for doing this are described in Chapter {ref}`ch6`.

\subsection*{Theoretical Models}
Theoretical models are part of familiar theoretical physics.
All analytical theories in physics are constructed out of basic building blocks -- definitions, axioms, hypotheses, principles, and so on, followed by analytical derivation from these basic starting points.
Because all the elements of theories are constructs of human imagination, the theories themselves and the results of the theories are similarly imaginary constructs.
```{figure} figures/ch4/Fig4_7.png
:A graph showing the use of line segments.
:Fig4_7
```
Their relevance to actual systems must be evaluated through experiments.
Let us illustrate the situation by using a particular example.
Consider a system in which we can release a steel ball bearing to fall freely under gravity, and we measure its time of fall from various heights.
If we wished to construct an empirical model of this system, we could simply measure the time of fall over a number of different distances and graph the result, which would look something like Figure {ref}`Fig4_8`.
We could then use one of the techniques from the preceding section to obtain an empirical model for the system.
If we wished to construct a theoretical model of the situation, however, our approach would be completely different.
We would have to choose a set of basic axioms or hypotheses from which we would derive the required results.
For example, we might decide to use as a basic hypothesis a presumed value for the acceleration of the ball bearing:

```math
a=9.8 \mathrm{m/s^2}
```

Notice that this hypothesis already contains several assumptions about the system, thereby starting our process of constructing an invented model.
By choosing a constant value for the acceleration, we are implicitly neglecting the presence of air resistance.
We have every right to do so.
The model is
ours; we are free to construct it in any way we please.
Whether that assumption makes it a \textit{good} model we may not yet be able to tell.
As a second example, we are also neglecting effects associated with general relativity; whether this is a serious defect also remains to be seen.
We should try to estimate in advance the validity of the assumptions that are built into the model, but we are often limited in our ability to do this.
There is always some point at which we have to decide to start experimenting on the basis of the model as it is and to rely on the experimental results to tell us if further refinement of the model is necessary.
```{figure} figures/ch4/Fig4_8.png
:Fig4_8
```

We are now ready to proceed with the development of our theoretical model.
By integration we obtain

```math
v=9.8t \hspace{0.25in} {\textrm assuming} \hspace{0.25in} v=0 \textrm{at} t=0
```

\noindent and

```math
x=\frac{9.8}{2}t^2 \hspace{0.25in} {\textrm assuming} \hspace{0.25in} x=0 \textrm{at} t=0
```

\noindent or

```math
t=\left(\frac{1}{4.9}\right)^{1/2}x^{1/2}
```

\noindent We have chosen to write the final equation in a form in which `t` is expressed as a function of `x` because this corresponds to the way in which the experiment was set up.
We chose `x` as the input, or independent, variable and measured `t` as the dependent, or output, variable.
So we want our equation to tell us `t` as a function of `x`.

In the course of the derivation, all the assumptions that we insert constitute further components of the model.
The final result for the measurable variable, the time of fall, is thus a property of the model.
Its applicability to the system is the next topic of investigation.

## Testing Theoretical Models\label{sec4_3}
Consider actual measurements for the free-fall experiment.
We have treated the distance of fall as the independent, or input, variable for which we chose the values; the time of fall is then the dependent, or output, variable of which the system gives us the values.
The results of the experiment are shown in Table {ref}`table4_2`.
In this experiment, the measurements of the distance of fall could be made much more precisely than those of the time of fall.
For simplification, we consider the uncertainty in the `x` values to be negligible.
Normally, we would have to include the uncertainty in all the measured quantities.

```{list-table}
:Experimentally Measured Time of Fall Versus Distance for a Freely Falling Object.
:table4_2
:header-rows: 1

* - \hline
    Distance, `x` (m)
  - Time, `t` (s)


    \hline
    `0.1`
  - `0.148\pm 0.01`


    `0.2`
  - `0.196`


    `0.3`
  - `0.244`


    `0.4`
  - `0.290`


    `0.5`
  - `0.315`


    `0.6`
  - `0.352`


    `0.7`
  - `0.385`


    `0.8`
  - `0.403`


    \hline
```
The measurements given in Table {ref}`table4_2` describe the behavior of the system.
We also have the behavior of the model, in the form of the function that was the outcome of the analytical derivation:

```math
t=\left(\frac{1}{4.9}\right)^{1/2}x^{1/2}
```

\noindent The task is somehow to compare these two, but it is not at all clear how that should be done.
One simple suggestion is to insert the various values of `x` in the equation and calculate corresponding values of `t`.
We could then compare these with the measured values.
If they agreed exactly, we could perhaps be confident that the system and the model were in correspondence.
The
probability of that happening, however, is minute; apart from anything else, the presence of uncertainty in the measurements eliminates the possibility of exact correspondence.
The major point, though, is that the model is most unlikely to be totally free of systematic defects and deficiencies.
It is one of the principal purposes in experimenting to detect these discrepancies and deal with them constructively.
The possibility of doing this effectively by using simple arithmetic comparison is small.
Much more significant for our purpose is the \textit{overall} behavior of the system; the best
way to view that behavior is on a graph.

The graph of our observations; shown in Figure {ref}`Fig4_9`(a) consists of a series of points.
The graph of the model's behavior is a curve, which is shown in Figure {ref}`Fig4_9`(b).
The two graphs together give us a visual impression of the relationship between the properties of the system and those of the model.
The comparison would be more detailed yet if we could pick up one of the graphs
and lay it over the other.
By doing so, we obtain the composite diagram shown in Figure {ref}`Fig4_9`(c).
Notice that this diagram has two different components: (1) points representing the properties of the system, and (2) a line corresponding to the analytical function that belongs to the model.

At last we can make a detailed comparison between the overall properties of the system and those of the model\index{comparison between models and systems `ff`}.
By straightforward visual inspection, we can say that the model and the system are in correspondence, or are divergent, or whatever.
We list the various possibilities in more detail in
Chapter {ref}`ch6`.
For the present, we must note carefully the kind of statement we are able to make at the end of an experiment.
We can say only that the behavior of the model and of the system were in correspondence (or were not) to such and such extent.
It is pointless to agonize over whether a theory is ``true'', ``correct'', or ``wrong,'' or whatever.
As was mentioned when we first discussed the nature of models, we should avoid using such terms, even if we are sure we understand the situation.
Others may be less clear about the use of words than we are, and there are too many opportunities to be misunderstood.
It is far better to categorize a theory or model as ``satisfactory'' or ``good enough,'' or some similar phrase, because all such decisions are relative to the purposes we have in mind.
```{figure} figures/ch4/Fig4_9.png
:The process of comparing the properties of a real system with the properties of a model.
:Fig4_9
```

For example, our simple model of constant acceleration under gravity is perfectly satisfactory for finding the depth of a well by dropping a stone down in it, but it is not satisfactory for calculating the trajectory of a space vehicle en route to the moon.
If that were our purpose, we would have to construct a more refined theory until we had one that is good enough for that purpose.
Even then, we would find that a theory adequate for moon rockets is inadequate to describe the motion of the planet Mercury.
For that, as was mentioned earlier, the theory of Newtonian gravitation must be replaced by Einstein's theory of general relativity.
And the adequacy of Einstein's theory for describing Mercury's orbit (at a particular level of precision) does not ``prove'' that it is true or correct, simply that it is good enough for that purpose.
Equally, the presence of Einstein's theory does not discredit either Newton's theory of gravitation or our simple model of constant acceleration
under gravity.
Most people do not measure the depth of wells by using Einstein's relativity theory.
In general, we use a particular theory because it is good enough for the purposes in hand.
If increased precision is desired at any time, the necessary refinements can be introduced as required (unless, of course, we are working at the limits of knowledge in a particular area, and the chief obstacle is the absence of an improved theory).

Because we are no longer going to use the misleading concept of the ``truth'' or ``correctness'' of theories and models, we shall be dependent on our own decision that a chosen model is good enough, or not, for our purpose.
One of the primary aims of experiment design is to test the models we use and check their suitability.
If it is properly planned, the experiment itself will tell us whether the model or theory is good enough.

In passing, we can note one interesting point of philosophy.
Even when our system and model seem to be in complete correspondence, we have to be careful about stating the outcome.
All we can say is that, at a particular level of precision, we have failed to observe any discrepancy between the system
and the model.
It is possible to be more assertive if we are sure that the properties of the model and system are in \textit{disagreement} by an amount clearly in excess of the measurement uncertainty; we can say definitely that the model is not in correspondence with the system.
We can say that we have ``proved'' the theory to be ``wrong'' -- although, even in this case, it would be better to call it ``unsuitable'' or ``inadequate.''

Before proceeding, we must note that this process of comparing systems and models depends on our ability to draw the graphs of the functions that appear in the models.
At one time this presented substantial difficulties for even simple functions, such as parabolas, and often insuperable difficulties for more complicated functions.
Now, the use of computers allows us to make the comparison directly.
We can display the graph of our experimental values along with the graph of the function that appears in the model, and judge immediately how well the two correspond.
The ways in which common spreadsheet programs can be made to do this are described in Chapter {ref}`ch6`.

Even when taking advantage of the marvelous opportunities offered by computers, we must not neglect the development of our own personal expertise in experimenting.
First, when no computer is available, we are forced to rely on our own resources.
Second, even when we are engaged in computer-based evaluation of experiments, we stand the chance of obtaining completely meaningless results unless we are clearly aware of every detail of the computations that are being carried out invisibly by the computer.
To develop our personal expertise, we must turn our attention (as was recommended earlier for calculations on frequency distributions and standard deviations) to the old-fashioned procedures that still constitute the basis of almost all experimenting.

If we are restricted to pencils, graph paper, and rulers, we are virtually compelled to compare the model with the system by using the only function whose graph is easy to draw -- a straight line.
(For fairly obvious reasons we exclude from consideration that other simple graph, the circle.) A large
amount of experimental analysis is still carried out in linear form, and even if the graphical techniques are cumbersome and tedious in comparison with the ease of computer-assisted evaluation, the methods are sufficiently powerful, important, and commonly used that we must become familiar with them.

## Use of Straight Line Analysis\label{sec4_4}
The objective is to arrange the plotting process so that the behavior of the system and the model are represented on a graph in linear form.

Consider the function for the time of free fall under gravity

```math
t=\left(\frac{1}{4.9}\right)^{1/2}x^{1/2}
```

This function, when plotted on an `x`, `t` graph has the shape of a parabola.
Consequently, if we were to plot the measurements of `x` and `t`, intending to compare them with the graph of the function in the model, it would be almost impossible to judge visually whether our results were compatible with a parabola.
Suppose, however, we were to plot as variables, not `t` versus `x` but `t`
versus `x^{1/2}`.
The equation that appears in the model

```math
t=0.4515\left(x)^{1/2}\right)
```

\noindent would then take the form of a straight line, whose equation we write

```math
\mathrm{vertical variable} = \textrm{slope }\times \textrm{horizontal variable}
```

\noindent in which the vertical and horizontal variables will be given by

```math
\mathrm{vertical variable} = t
```

```math
\mathrm{horizontal variable} = x^{1/2}
```

\noindent and

```math
\mathrm{slope} = 0.4515
```

The experimental values of `x^{1/2}` and `t` are given in Table {ref}`table4_3` and are plotted
in Figure {ref}`Fig4_10`.
This graph also contains the line representing the function; the resulting simplification is immediately obvious.
The whole process of comparison is facilitated, and we can identify immediately the degree of correspondence between the model and the system.

```{list-table}
:Experimentally Measured Time of Fall Versus Square Root of Distance for a Freely Falling Object.
:table4_3
:header-rows: 1

* - \hline
    Distance, `x` (m)
  - (Distance)`^{1/2}`, `x^{1/2} \left(\textrm{m}^{1/2}\right)`
  - Time, `t` (s)


    \hline
    `0.1`
  - `0.316`
  - `0.148\pm 0.01`


    `0.2`
  - `0.447`
  - `0.196`


    `0.3`
  - `0.548`
  - `0.244`


    `0.4`
  - `0.632`
  - `0.290`


    `0.5`
  - `0.707`
  - `0.315`


    `0.6`
  - `0.775`
  - `0.352`


    `0.7`
  - `0.837`
  - `0.385`


    `0.8`
  - `0.894`
  - `0.403`


    \hline
```
```{figure} figures/ch4/Fig4_10.png
:Comparison between the properties of a model and of the system when expressed in (a) non-linear form and (b) straight-line form.
:Fig4_10
```

In this example we chose the time of fall as the output variable and the distance of fall as the input variable.
This choice caused us to plot the variables as `t` and `x^{1/2}`.
The process would have been equally effective and probably more convenient if we had plotted (`t^2` versus `x` instead of `t` versus `x^{1/2}`.
The slope would have been different, but the opportunity to compare the
model and the system would have been equally good.
In an experiment, there may be several equivalent ways to plot the variables in the form of a straight line.
Sometimes one is more convenient than another; sometimes one gives us a better basis for comparing the system and the model.
We have to make a decision each time on the basis of the particular circumstances.

Finally, in this example the properties of the model were completely specified, and the line on Figure {ref}`Fig4_10` representing the model's behavior is unique.
The situation is slightly different if the model contains quantities of which we do not know the value; this is the topic of the following section.

## Case of Undetermined Constants\label{sec4_5}
Suppose we are doing an experiment on a spring to determine the extension under various loads (again assuming that only the extension has an uncertainty that needs to be taken into account).
Suppose we are aware of a proposal (due to Hooke) that extension `x` can be considered to be proportional to
load `W`.
This proposal, expressed in mathematical form as

```math
x = \mathrm{constant} \times W
```

\noindent constitutes an invented model of the system.
Assume that we wish to test this model against the system.
The only trouble is that (unlike the former example of the falling ball in which the model contained the known value of the gravitational acceleration) we may not know the value of the constant (the ``springiness'') that appears in the equation defining the model.
Suppose we have made measurements of extension versus load and plotted them in Figure {ref}`Fig4_11`(a), What are we to do to represent the behavior of the model?
The equation

```math
x = \mathrm{constant} \times W
```

\noindent really represents the infinite set of lines on the `W - x` plane that have all the values of slope, from zero to infinity, that correspond to the infinite range of possibilities for the value of the constant.
Some of these lines are represented in Figure {ref}`Fig4_11`b), What, then constitutes the outcome of our comparison?
Laying one graph on top of the other produces the diagram shown in Figure {ref}`Fig4_11`(c) and provides us with the opportunity to choose a line that is compatible with the experimental points.

But which line or lines are we to choose?
Clearly the lines `OA` and `OB` have no obvious relevance to the observations and can be disregarded, We can, on the other hand, identify a certain bundle of lines that fall within the region of uncertainty of the measured points.
We can estimate visually the
edges of this bundle; they are represented by the lines `OC` and `OD`, Within these limits all the lines have some degree of consistency with the observations, but no single line stands out as uniquely suitable.
All we can say for the moment is that the observations are compatible with the model over a certain \textit{range} of slopes, This means that there is a certain range of values of ``springiness'' (within the model) that are consistent with the system.
The conclusion is, then, that if we have an initially undetermined constant in the model, the experimenting process can be used to determine, within a certain interval, the value that is appropriate for the system, This is the normal way of determining experimental values of physical quantities.
```{figure} figures/ch4/Fig4_11.png
:Use of graphs to avoid errors in slope measurements.
:Fig4_11
```


The process is so commonly used because, in addition to the almost necessary use of the graph in comparing models and systems, graphical methods of obtaining values of experimental constants offer so much additional advantage in increased precision that their use is compellingly attractive.
The opportunities for error when using algebraic computation alone without graphical checking are great.
For example, suppose we are trying to obtain a measured value such as the electrical resistance of a resistor from the variation of the voltage across it with current through it.
We make pairs of measurements of `I` and `V`, and we use the relationship `V = IR` directly to obtain a value of `R` from each pair of `I`, `V` values by purely algebraic means.
We then hope to obtain an accurate value for `R` by calculating the average of all the resulting `R` values.
This approach is deficient in many ways, Basically it fails to satisfy the primary requirement -- to compare the properties of the system and the model -- and the consequences for the accuracy of the `R` value can be serious.
If all our pairs of `I`, `V` values gave the same, or nearly the same, value for `R`, we might feel confident in our measurement of `R`, even without drawing the `I`, `V` graph.
In the much more likely event that the `R` values do not all turn out to be the same, we have no way of interpreting the variations without the graph.

We might, for example, encounter a case in which, as plotting the graph would have revealed the points show more scatter than we expected [see Figure {ref}`Fig4_11`(a)]. Using a graphical approach, we could still choose a suitable straight line (passing through the origin, if we are sure of the origin as a measured point) and feel reasonably confident about the `R` value obtained from the slope.
Our confidence is justified because the appearance of the graph convinces us that we are dealing with simple scatter about a basically linear variation.
Our nongraphical, algebraic calculation would, on the other hand, give us values that correspond to the slopes of the lines `OA`, `OB`, `OC`, and so on.
In a simple table of values, the resulting variability would make no sense at all, and we would gain no insight into what is happening.

As a more significant illustration of the inadequacy of an algebraic, nongraphical approach, consider a case in which some failure of correspondence between the model and the system gives rise either to an unexpected intercept or to deviation from linearity beyond a certain range.
Using a graphical approach, we can easily detect and compensate for these discrepancies between model and system.
In the first case the graph enables us to obtain a reliable value of `R` from the slope, which is in such cases unaffected by the presence of the intercept.
In the second case we obtain the `R` value
from the slope of the linear portion of the `I`, `V` variation, rightly dismissing the nonlinear points as lying outside the scope of the model.
The opportunity to make these judgments can come only from visual inspection of the graph, which makes the situation clear at a glance.
As before, nongraphical, algebraic calculation from the pairs of `I`, `V` values alone yields values of `R` corresponding [see Figures {ref}`Fig4_11`(b) and (c)] to the slopes of `OA`, `OB`, `OC`, and so
on.
These slopes have nothing to do with the slope we want; if we were to include them in an average of algebraically calculated quantities, we would succeed only in introducing error into our answers.

As we ensure in these ways that the final answer is free from such sources of systematic error, it does not matter whether or not we know the origin of the discrepancy.
For the purposes of obtaining the value of the quantity under study, it is sufficient at this stage merely to identify the existence
of the discrepancy and to ensure that it is not permitted to introduce errors into the answer.
We can later consider possible sources of the discrepancy.

We have discussed the process of obtaining values for initially undetermined constants in terms of slopes only.
In principle, since a line has two degrees of freedom on a plane, it is possible to obtain from it two pieces of information independently, such as a slope and an intercept.
Because of this, an experiment can be made to yield values for two separate quantities that appear in a model.
Specific methods of doing this in actual practice are discussed in Chapter {ref}`ch6`.
